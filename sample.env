# AWS_REGION is used by Spark
AWS_REGION=us-east-1

# Minio
MINIO_REGION=${AWS_REGION}
MINIO_USER_USERNAME=minioadmin
MINIO_USER_PASSWORD=minioadmin

# Used by pyIceberg
AWS_DEFAULT_REGION=us-east-1
# AWS Credentials (this can use minio credential, to be filled in later)
AWS_ACCESS_KEY_ID=miniouser
AWS_SECRET_ACCESS_KEY=miniouser
# If using Minio, this should be the API address of Minio Server
AWS_S3_ENDPOINT=http://minio:9000
# Location where files will be written when creating new tables
ICEBERG_BUCKET=iceberg
DELTA_LAKE_BUCKET=delta
ICEBERG_ENDPOINT=s3a://${ICEBERG_BUCKET}/
DELTA_LAKE_ENDPOINT=s3a://${DELTA_LAKE_BUCKET}/

# Nessie Catalog
NESSIE_URI=http://nessie:19120/api/v1

# Trino
TRINO_URL=https://trino:8080
# TRINO_URL=https://trino:8443
# KEYSTORE_PASSWORD=secure123

# PostgreSQL
DB_NAME=public
DB_USERNAME=pgadmin
DB_PASSWORD=pgadmin
DB_JDBC_URI=jdbc:postgresql://postgres:5432/${DB_NAME}

# Airflow
# Refer to https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user
# AIRFLOW_UID=50000 # set this if running on non-linux system Ex. MacOS, Windows
AIRFLOW_UID=1000 # If running on linux run 'id -u' and paste the value here
AIRFLOW_API_SERVER_URL=http://airflow-apiserver:8088/execution/
AIRFLOW_USER_USERNAME=airflow
AIRFLOW_USER_PASSWORD=airflow

