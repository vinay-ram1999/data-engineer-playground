{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f12c0e9",
   "metadata": {},
   "source": [
    "**Note:** This is a DDL notebook. Run this only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c77d34-e043-4381-83e6-993b70310013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: iceberg.expire.tables.gc.enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "org.slf4j#slf4j-simple added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a88af8c6-2b44-4dea-90b8-b99093b22c3c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.8.1 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.83.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.29.52 in central\n",
      "\tfound org.slf4j#slf4j-simple;2.0.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.365 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.8.1/iceberg-spark-runtime-3.5_2.12-1.8.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.8.1!iceberg-spark-runtime-3.5_2.12.jar (5866ms)\n",
      "downloading https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.5_2.12/0.83.1/nessie-spark-extensions-3.5_2.12-0.83.1.jar ...\n",
      "\t[SUCCESSFUL ] org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.83.1!nessie-spark-extensions-3.5_2.12.jar (316ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.29.52/bundle-2.29.52.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#bundle;2.29.52!bundle.jar (75011ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-simple/2.0.7/slf4j-simple-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-simple;2.0.7!slf4j-simple.jar (64ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (138ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.365/aws-java-sdk-bundle-1.12.365.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.365!aws-java-sdk-bundle.jar (35991ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (49ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (122ms)\n",
      ":: resolution report :: resolve 6192ms :: artifacts dl 117565ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.365 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.8.1 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.83.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.slf4j#slf4j-simple;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.29.52 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.365] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   8   |   8   |   1   ||   8   |   8   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a88af8c6-2b44-4dea-90b8-b99093b22c3c\n",
      "\tconfs: [default]\n",
      "\t8 artifacts copied, 0 already retrieved (977108kB/1533ms)\n",
      "25/04/25 06:51:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is up and running!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/workspace/seed\")\n",
    "\n",
    "from seed.iceberg_setup import conf\n",
    "\n",
    "spark: SparkSession = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark is up and running!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88e98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|      dev|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a namespace in nessie catalog\n",
    "\n",
    "# spark.sql(\"DROP NAMESPACE IF EXISTS nessie.dev;\")\n",
    "\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.dev;\")\n",
    "\n",
    "spark.sql(\"SHOW NAMESPACES FROM nessie\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24fedce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "25/04/25 06:51:37 WARN NessieUtil: The Iceberg property 'gc.enabled' and/or 'write.metadata.delete-after-commit.enabled' is enabled on table 'dev.names' in NessieCatalog. This will likely make data in other Nessie branches and tags and in earlier, historical Nessie commits inaccessible. The recommended setting for those properties is 'false'. Use the 'nessie-gc' tool for Nessie reference-aware garbage collection.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|vinay|\n",
      "+-----+\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|      dev|    names|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a test table in the namespace\n",
    "\n",
    "spark.sql(\"CREATE OR REPLACE TABLE nessie.dev.names (name STRING) USING iceberg TBLPROPERTIES ('gc.enabled' = 'true');\")\n",
    "\n",
    "spark.sql(\"INSERT INTO nessie.dev.names VALUES ('vinay');\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM nessie.dev.names;\").show()\n",
    "\n",
    "spark.sql(\"SHOW TABLES FROM nessie;\").show()\n",
    "\n",
    "# spark.sql(\"DROP TABLE nessie.dev.names PURGE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab3143b4-0a1c-4905-ad08-fc20f35eed25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 06:51:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+--------+--------+---------+---------+\n",
      "|   FL_DATE|DEP_DELAY|ARR_DELAY|AIR_TIME|DISTANCE| DEP_TIME| ARR_TIME|\n",
      "+----------+---------+---------+--------+--------+---------+---------+\n",
      "|2006-01-01|        5|       19|     350|    2475| 9.083333|12.483334|\n",
      "|2006-01-02|      167|      216|     343|    2475|11.783334|15.766666|\n",
      "|2006-01-03|       -7|       -2|     344|    2475| 8.883333|12.133333|\n",
      "|2006-01-04|       -5|      -13|     331|    2475| 8.916667|    11.95|\n",
      "|2006-01-05|       -3|      -17|     321|    2475|     8.95|11.883333|\n",
      "+----------+---------+---------+--------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- DEP_DELAY: short (nullable = true)\n",
      " |-- ARR_DELAY: short (nullable = true)\n",
      " |-- AIR_TIME: short (nullable = true)\n",
      " |-- DISTANCE: short (nullable = true)\n",
      " |-- DEP_TIME: float (nullable = true)\n",
      " |-- ARR_TIME: float (nullable = true)\n",
      "\n",
      "+----------+----------+--------+\n",
      "|  min_date|  max_date|num_rows|\n",
      "+----------+----------+--------+\n",
      "|2006-01-01|2006-02-28| 1000000|\n",
      "+----------+----------+--------+\n",
      "\n",
      "+----------+--------+\n",
      "|   FL_DATE|num_rows|\n",
      "+----------+--------+\n",
      "|2006-01-01|   17618|\n",
      "|2006-01-02|   19156|\n",
      "|2006-01-03|   19290|\n",
      "|2006-01-04|   18869|\n",
      "|2006-01-05|   19534|\n",
      "|2006-01-06|   19553|\n",
      "|2006-01-07|   16236|\n",
      "|2006-01-08|   18506|\n",
      "|2006-01-09|   19483|\n",
      "|2006-01-10|   18541|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read flights data from parquet file\n",
    "\n",
    "df = spark.read.parquet(\"s3a://seed/flights-1m.parquet\")\n",
    "df.show(5)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.createOrReplaceTempView(\"raw_flights\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        MIN(FL_DATE) AS min_date,\n",
    "        MAX(FL_DATE) AS max_date,\n",
    "        COUNT(*) AS num_rows\n",
    "    FROM raw_flights;\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        FL_DATE,\n",
    "        COUNT(*) AS num_rows\n",
    "    FROM raw_flights\n",
    "    GROUP BY FL_DATE\n",
    "    ORDER BY FL_DATE\n",
    "    LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36fbfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 06:51:48 WARN NessieUtil: The Iceberg property 'gc.enabled' and/or 'write.metadata.delete-after-commit.enabled' is enabled on table 'dev.flights' in NessieCatalog. This will likely make data in other Nessie branches and tags and in earlier, historical Nessie commits inaccessible. The recommended setting for those properties is 'false'. Use the 'nessie-gc' tool for Nessie reference-aware garbage collection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+--------+--------+--------+---------+\n",
      "|   FL_DATE|DEP_DELAY|ARR_DELAY|AIR_TIME|DISTANCE|DEP_TIME| ARR_TIME|\n",
      "+----------+---------+---------+--------+--------+--------+---------+\n",
      "|2006-02-28|       -7|        0|     340|    2475|8.883333|12.166667|\n",
      "|2006-02-28|        6|      -14|     270|    2475|     9.6|17.516666|\n",
      "|2006-02-28|       -9|      -13|     338|    2475|   11.85|14.833333|\n",
      "|2006-02-28|        6|       -9|     274|    2475|    12.6|20.666666|\n",
      "|2006-02-28|      -12|       -1|     498|    3784|9.883333|14.666667|\n",
      "+----------+---------+---------+--------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create flights table from parquet file\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS nessie.dev.flights \n",
    "    USING iceberg\n",
    "    PARTITIONED BY (fl_date)\n",
    "    TBLPROPERTIES ('gc.enabled' = 'true')\n",
    "    AS\n",
    "    SELECT\n",
    "        *\n",
    "    FROM raw_flights;\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM nessie.dev.flights LIMIT 5;\").show()\n",
    "\n",
    "# spark.sql(\"DROP TABLE nessie.dev.flights PURGE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71448e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
