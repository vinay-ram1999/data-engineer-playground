{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c77d34-e043-4381-83e6-993b70310013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      "org.slf4j#slf4j-simple added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5a83bb84-b7c0-4387-8455-3b4b7666d5fc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.2 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.83.1 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-simple;2.0.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.365 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.4!hadoop-aws.jar (268ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.365/aws-java-sdk-bundle-1.12.365.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.365!aws-java-sdk-bundle.jar (84595ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (238ms)\n",
      ":: resolution report :: resolve 5103ms :: artifacts dl 85111ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.365 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.2 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.83.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.slf4j#slf4j-simple;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.365] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   3   |   3   |   2   ||   15  |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5a83bb84-b7c0-4387-8455-3b4b7666d5fc\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 12 already retrieved (304016kB/178ms)\n",
      "25/04/18 07:23:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+-----------------+\n",
      "|             name|\n",
      "+-----------------+\n",
      "|      Alex Merced|\n",
      "|Dipankar Mazumdar|\n",
      "|     Jason Hughes|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "NESSIE_URI      = os.environ[\"NESSIE_URI\"]      # e.g. http://nessie:19120/api/v1\n",
    "WAREHOUSE       = os.environ[\"WAREHOUSE\"]       # e.g. s3a://lakehouse/\n",
    "AWS_ACCESS_KEY_ID  = os.environ[\"AWS_ACCESS_KEY_ID\"]  # from .env\n",
    "AWS_SECRET_ACCESS_KEY  = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "AWS_S3_ENDPOINT = os.environ[\"AWS_S3_ENDPOINT\"] # e.g. http://minio:9000\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set(\n",
    "        'spark.jars.packages',\n",
    "        'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,'\n",
    "        'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.83.1,'\n",
    "        'software.amazon.awssdk:bundle:2.17.178,'\n",
    "        'software.amazon.awssdk:url-connection-client:2.17.178,'\n",
    "        'org.slf4j:slf4j-simple:2.0.7,'\n",
    "        'org.apache.hadoop:hadoop-aws:3.3.4,'\n",
    "        'com.amazonaws:aws-java-sdk-bundle:1.12.365'\n",
    "        )\n",
    "        .set(\n",
    "            'spark.sql.extensions',\n",
    "            'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "            'org.projectnessie.spark.extensions.NessieSparkSessionExtensions'\n",
    "        )\n",
    "\n",
    "        # Nessie catalog\n",
    "        .set('spark.sql.catalog.nessie',            'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri',        NESSIE_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref',        'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl','org.apache.iceberg.nessie.NessieCatalog')\n",
    "\n",
    "        # Iceberg S3FileIO (AWS SDK) â†’ must supply keys + region\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint',        AWS_S3_ENDPOINT)\n",
    "        .set('spark.sql.catalog.nessie.s3.path-style-access','true')\n",
    "        .set('spark.sql.catalog.nessie.s3.access-key',      AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.sql.catalog.nessie.s3.secret-key',      AWS_SECRET_ACCESS_KEY)\n",
    "        .set('spark.sql.catalog.nessie.s3.region',          os.environ.get(\"AWS_REGION\", \"us-east-1\"))\n",
    "\n",
    "        # Warehouse & IO impl\n",
    "        .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.nessie.io-impl',   'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "\n",
    "        # Hadoop S3A (for any s3a:// URI)\n",
    "        .set('spark.hadoop.fs.s3a.endpoint',               AWS_S3_ENDPOINT)\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access',      'true')\n",
    "        .set('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "        .set('spark.hadoop.fs.s3a.access.key',             AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key',             AWS_SECRET_ACCESS_KEY)\n",
    ")\n",
    "\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "print(\"Spark Running\")\n",
    "\n",
    "\n",
    "## Create a Table\n",
    "spark.sql(\"CREATE OR REPLACE TABLE nessie.names (name STRING) USING iceberg;\").show()\n",
    "\n",
    "## Insert Some Data\n",
    "spark.sql(\"INSERT INTO nessie.names VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\n",
    "\n",
    "\n",
    "## Query the Data\n",
    "spark.sql(\"SELECT * FROM nessie.names;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7b813b-62f0-4b14-af1a-7661d6e6e0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |    names|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES FROM nessie;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab3143b4-0a1c-4905-ad08-fc20f35eed25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/18 07:23:54 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+--------+--------+---------+---------+\n",
      "|   FL_DATE|DEP_DELAY|ARR_DELAY|AIR_TIME|DISTANCE| DEP_TIME| ARR_TIME|\n",
      "+----------+---------+---------+--------+--------+---------+---------+\n",
      "|2006-01-01|        5|       19|     350|    2475| 9.083333|12.483334|\n",
      "|2006-01-02|      167|      216|     343|    2475|11.783334|15.766666|\n",
      "|2006-01-03|       -7|       -2|     344|    2475| 8.883333|12.133333|\n",
      "|2006-01-04|       -5|      -13|     331|    2475| 8.916667|    11.95|\n",
      "|2006-01-05|       -3|      -17|     321|    2475|     8.95|11.883333|\n",
      "+----------+---------+---------+--------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"s3a://seed/flights-1m.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039014d-5e0b-4a56-b7f5-d4349b10c0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
